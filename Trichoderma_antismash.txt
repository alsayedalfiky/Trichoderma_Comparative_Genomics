#!/bin/bash

# =============================================================================
# Filtering GenBank Assembly Files to Retain Coding Sequences (CDS)
# Filter GenBank file to keep only contigs with CDS features
# =============================================================================

# Set paths for T. virens - corrected directory structure
INPUT_FILE="/home/bioalfiky25/Trichoderma_pan_genome_clean_run/04_Functional_Annotation/Input_data/T_virens/Trichoderma_virens_G-41.gbk"
OUTPUT_FILE="/home/bioalfiky25/Trichoderma_pan_genome_clean_run/04_Functional_Annotation/Input_data/T_virens/Trichoderma_virens_G-41.properly_filtered.gbk"

# Create the Python filtering script
cat > filter_genbank.py << 'EOF'
#!/usr/bin/env python3
from Bio import SeqIO
import sys

def filter_genbank(input_file, output_file):
    """
    Reads a GenBank file and writes a new one, containing only contigs (sequences)
    that have at least one CDS feature.
    """
    print(f"Reading records from {input_file}...")
    
    # Parse all records from the input file
    all_records = list(SeqIO.parse(input_file, "genbank"))
    print(f"Found {len(all_records)} total contigs.")
    
    # Filter records: keep only those with at least one CDS feature
    filtered_records = []
    for record in all_records:
        cds_count = sum(1 for feature in record.features if feature.type == "CDS")
        if cds_count > 0:
            filtered_records.append(record)
            print(f"  Keeping contig '{record.id}' with {cds_count} CDS features.")
        else:
            print(f"  Discarding contig '{record.id}' with 0 CDS features.")
    
    print(f"Keeping {len(filtered_records)} contigs with CDS features.")
    
    # Write the filtered records to a new GenBank file
    if filtered_records:
        SeqIO.write(filtered_records, output_file, "genbank")
        print(f"Successfully wrote filtered records to {output_file}")
    else:
        print("Error: No records with CDS features were found!", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: python filter_genbank.py <input.gbk> <output.gbk>")
        sys.exit(1)
    
    input_gbk = sys.argv[1]
    output_gbk = sys.argv[2]
    filter_genbank(input_gbk, output_gbk)
EOF

# Make the script executable and run it
chmod +x filter_genbank.py
python filter_genbank.py "$INPUT_FILE" "$OUTPUT_FILE"

# =============================================================================
# Run antiSMASH analysis
# =============================================================================

#!/bin/bash

# Set paths for T. virens - corrected directory structure
INPUT_GBK="/path/04_Functional_Annotation/Input_data/T_virens/Trichoderma_virens_G-41.properly_filtered.gbk"
OUTPUT_DIR="/path/04_Functional_Annotation/Output_data/antiSMASH_results/T_virens"

# Create output directory
mkdir -p "$OUTPUT_DIR"

# Run antiSMASH with consistent parameters
antismash \
    "$INPUT_GBK" \
    --taxon fungi \
    --cb-general \
    --cc-mibig \
    --cpus 12 \
    --output-dir "$OUTPUT_DIR"

echo "antiSMASH analysis completed for T. virens"

# =============================================================================
# Analyze antiSMASH results
# =============================================================================

# Get full product list for each hybrid region
jq -r '.records[].features[] | select(.type == "region") | {region: .qualifiers.region_number[0], product: .qualifiers.product[]} | "\(.region)\t\(.product)"' *.json \
    | sort -u \
    | awk '
    {
        products[$1] = (products[$1] ? products[$1] "; " : "") $2
    }
    END {
        for (region in products) {
            split(products[region], arr, "; ");
            unique_count = 0;
            delete seen;
            for (i in arr) {
                if (!seen[arr[i]]++) unique_count++;
            }
            if (unique_count > 1) {
                print "Region " region " has " unique_count " unique products: " products[region]
            }
        }
    }'

# Harmonized count of hybrid regions
jq -r '.records[].features[] | select(.type == "region") | {region: .qualifiers.region_number[0], product: .qualifiers.product[]} | "\(.region)\t\(.product)"' *.json \
    | sort -u \
    | awk '
    {
        products[$1] = (products[$1] ? products[$1] "; " : "") $2
    }
    END {
        for (region in products) {
            split(products[region], arr, "; ");
            delete seen;
            for (i in arr) seen[arr[i]]++;
            if (length(seen) > 1) print region;
        }
    }' | sort -u | wc -l

# Count all BGC types
jq -r '.records[].features[] | select(.type == "region") | .qualifiers.product[]' *.json | sort | uniq -c | sort -nr

# List all unique feature types
jq -r '.records[].features[].type' *.json | sort | uniq -c

# =============================================================================
# Extract proteins from antiSMASH GBK files
# =============================================================================

cd "/path/04_Functional_Annotation/Output_data/antiSMASH_results/T_virens"

# Create protein extraction script
cat > extract_proteins.py << 'EOF'
#!/usr/bin/env python3
import os
from Bio import SeqIO

# Automatically find all .gbk files in the current directory
gbk_files = [f for f in os.listdir('.') if f.endswith('.gbk')]
# Exclude the "properly_filtered" file if present
gbk_files = [f for f in gbk_files if 'properly_filtered' not in f]

# Set the output file name
output_faa = "BGC_proteins_validated.faa"

# Use a set to store unique protein IDs to avoid duplicates
extracted_proteins = set()

# Iterate through each region .gbk file
with open(output_faa, "w") as out_f:
    for gbk_file in gbk_files:
        for record in SeqIO.parse(gbk_file, "genbank"):
            for feature in record.features:
                if feature.type == "CDS" and "translation" in feature.qualifiers:
                    protein_id = feature.qualifiers.get("locus_tag", ["unknown"])[0]
                    if protein_id not in extracted_proteins:
                        translation = feature.qualifiers["translation"][0]
                        out_f.write(f">{protein_id}\n{translation}\n")
                        extracted_proteins.add(protein_id)

print(f"Extracted {len(extracted_proteins)} unique proteins to {output_faa}")
EOF

# Run the protein extraction
python extract_proteins.py

# Count extracted proteins
grep -c '>' BGC_proteins_validated.faa

# =============================================================================
# BLAST search against MIBiG database
# =============================================================================

cd "/path/04_Functional_Annotation/Output_data/antiSMASH_results/T_virens"

# Run BLASTp against MIBiG database
MIBIG_DB="/path/04_Functional_Annotation/Output_data/antiSMASH_results/mibig_prot_db/mibig_prot_db"

blastp \
    -query BGC_proteins_validated.faa \
    -db "$MIBIG_DB" \
    -out mibig_blast_final.tsv \
    -evalue 1e-5 \
    -num_threads 12 \
    -outfmt "6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore"

# Filter for top hits per query
awk '{if (!seen[$1]++) print}' mibig_blast_final.tsv > mibig_blast_final_top.tsv

# Count filtered proteins
echo "Number of significant hits:"
wc -l mibig_blast_final_top.tsv

# Filter by significance criteria
awk '$3 >= 30 && $11 <= 1e-10 && $4 >= 100' mibig_blast_final_top.tsv > mibig_blast_final_filtered.tsv

# Count filtered proteins
echo "Number of significant hits:"
wc -l mibig_blast_final_filtered.tsv

# =============================================================================
# Convert BLAST results to Excel format
# =============================================================================

cd "/path/04_Functional_Annotation/Output_data/antiSMASH_results/T_virens"

# Create conversion script
cat > blast_to_excel.py << 'EOF'
#!/usr/bin/env python3
import pandas as pd

# Read the filtered TSV file (no headers)
df = pd.read_csv('mibig_blast_final_filtered.tsv', sep='\t', header=None)

# Assign column names based on BLAST outfmt 6 format
blast_columns = [
    'Query ID',
    'Subject ID Full', 
    'Percentage Identity',
    'Alignment Length',
    'Mismatches',
    'Gap Opens',
    'Query Start',
    'Query End', 
    'Subject Start',
    'Subject End',
    'E-value',
    'Bit Score'
]
df.columns = blast_columns

# Function to extract clean MIBiG ID and annotation
def parse_mibig_id(full_id):
    parts = str(full_id).split('|')
    clean_id = parts[0] if parts else full_id
    annotation = parts[-2] if len(parts) > 2 else 'Unknown'
    return clean_id, annotation

# Apply parsing to create new columns
df[['MIBiG Subject ID', 'Annotation']] = df['Subject ID Full'].apply(
    lambda x: pd.Series(parse_mibig_id(x))
)

# Select only the columns you want in the final output
final_columns = [
    'Query ID',
    'MIBiG Subject ID',
    'Percentage Identity',
    'Alignment Length',
    'E-value',
    'Bit Score',
    'Annotation'
]
final_df = df[final_columns]

# Rename columns to match your desired headers exactly
final_df = final_df.rename(columns={
    'Percentage Identity': '% Identity',
    'Alignment Length': 'Alignment Length',
    'E-value': 'E-value', 
    'Bit Score': 'Bit Score'
})

# Save to Excel
final_df.to_excel('mibig_blast_filtered_final.xlsx', index=False, engine='openpyxl')

print("âœ… Excel file created: mibig_blast_filtered_final.xlsx")
print(f"ðŸ“Š Total high-confidence hits: {len(final_df)}")
print("ðŸ“‹ Columns included: Query ID, MIBiG Subject ID, % Identity, Alignment Length, E-value, Bit Score, Annotation")
EOF

# Run the conversion
python blast_to_excel.py

# =============================================================================
# Pfam domain analysis with HMMER
# =============================================================================

cd "/path/04_Functional_Annotation/Output_data/antiSMASH_results/T_virens"

# Run hmmscan against Pfam database
PFAM_DB="/path/04_Functional_Annotation/Output_data/antiSMASH_results/T_asperellum/Pfam-A.hmm"

hmmscan --cpu 12 --domtblout pfam_final.domtblout "$PFAM_DB" BGC_proteins_validated.faa > hmmscan_final.log

# Analyze hmmscan results

# Count how many significant domain hits were found in total
grep -v '^#' pfam_final.domtblout | awk '$7 < 1e-5' | wc -l

# Count how many unique proteins have at least one significant domain hit
grep -v '^#' pfam_final.domtblout | awk '$7 < 1e-5 {print $4}' | sort -u | wc -l